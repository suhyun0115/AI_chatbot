{"cells":[{"cell_type":"code","execution_count":null,"id":"02157866-4875-4c09-aaa1-3305ae154c33","metadata":{"id":"02157866-4875-4c09-aaa1-3305ae154c33"},"outputs":[],"source":["# !pip install transformers\n","# !pip install peft\n","# !pip install pandas\n","# !pip install bitsandbytes"]},{"cell_type":"code","execution_count":null,"id":"de3d4290-24d0-4c4e-9b75-c1bd7093cabb","metadata":{"id":"de3d4290-24d0-4c4e-9b75-c1bd7093cabb"},"outputs":[],"source":["import os\n","import transformers\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import AutoTokenizer, AutoModelForCausalLM , PreTrainedTokenizerFast, AdamW, BitsAndBytesConfig, GPT2LMHeadModel, LlamaTokenizer\n","from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n","import pandas as pd\n","import tqdm\n","import urllib.request\n","import bitsandbytes\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":null,"id":"9a9545ce-5c38-43ef-b7d7-d9f3d76e9f35","metadata":{"id":"9a9545ce-5c38-43ef-b7d7-d9f3d76e9f35"},"outputs":[],"source":["torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"id":"6c41c928-41c3-40c1-b63c-32c7f81a193f","metadata":{"colab":{"referenced_widgets":["e4defc313050440b8c4c24d5663227f2"]},"id":"6c41c928-41c3-40c1-b63c-32c7f81a193f","outputId":"b2ecb4f0-a9b3-4962-d988-d47512a6fa70"},"outputs":[{"name":"stderr","output_type":"stream","text":["The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'LlamaTokenizer'. \n","The class this function is called from is 'PreTrainedTokenizerFast'.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e4defc313050440b8c4c24d5663227f2","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# 토크나이저 초기화\n","tokenizer = PreTrainedTokenizerFast.from_pretrained('LDCC/LDCC-SOLAR-10.7B',  eos_token='</s>')\n","\n","# 모델 초기화\n","model_id = \"LDCC/LDCC-SOLAR-10.7B\"\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.float16\n",")\n","model = AutoModelForCausalLM.from_pretrained(model_id,\n","                                             quantization_config=bnb_config,\n","                                             torch_dtype=torch.float32,\n","                                             output_attentions=True)\n","# 수정된 LoRA 설정: 대상 모듈 이름을 수정함\n","lora_config = LoraConfig(\n","    r=8,  # LoRA 업데이트 행렬의 순위\n","    lora_alpha=32,  # LoRA 스케일링 팩터\n","    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # 수정된 대상 모듈 이름\n","    lora_dropout=0.1,  # LoRA 드롭아웃 비율\n","    bias=\"none\",  # LoRA의 편향 설정\n","    task_type=\"CAUSAL_LM\"  # 태스크 유형\n",")\n"]},{"cell_type":"code","execution_count":null,"id":"7f606e38-dff2-4383-9ef9-bf38e161a7b7","metadata":{"id":"7f606e38-dff2-4383-9ef9-bf38e161a7b7"},"outputs":[],"source":["class ChatDataset(Dataset):\n","    def __init__(self, train_data, tokenizer):\n","        self.train_data = train_data\n","        self.tokenizer = tokenizer\n","\n","    def __len__(self):\n","        return len(self.train_data)\n","\n","    def __getitem__(self, idx):\n","        question = self.train_data.Question.iloc[idx]\n","        answer = self.train_data.Answer.iloc[idx]\n","        bos_token = self.tokenizer.bos_token_id\n","        eos_token = self.tokenizer.eos_token_id\n","\n","        sent = self.tokenizer.encode('<usr>' + question + '<sys>' + answer, add_special_tokens=False)\n","        return torch.tensor([bos_token] + sent + [eos_token], dtype=torch.long)\n","\n","def collate_fn(batch):\n","    return torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=tokenizer.pad_token_id)\n","\n","df = pd.read_csv('last_df.csv')\n","\n","# 모델을 LoRA 파인 튜닝에 맞게 준비\n","model = prepare_model_for_kbit_training(model)\n","model = get_peft_model(model, lora_config)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","# 여러 GPU를 사용하기 위해 DataParallel 적용\n","if torch.cuda.device_count() > 1:\n","    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n","    model = torch.nn.DataParallel(model)\n","\n","model.to('cuda')  # 모델을 GPU로 이동\n","\n","batch_size = 10\n","EPOCHS = 10\n","steps = len(df) // batch_size + 1\n","PATIENCE = 2  # 성능 개선이 없는 경우, 2 에포크 이후 학습 중단\n","best_loss = float('inf')\n","patience_counter = 0\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=3e-5, eps=1e-08)\n","\n","\n","# 데이터셋과 데이터 로더 준비\n","chat_dataset = ChatDataset(df, tokenizer)\n","data_loader = DataLoader(chat_dataset, batch_size=batch_size, collate_fn=collate_fn)\n","\n","# 옵티마이저 설정\n","optimizer = torch.optim.Adam(model.parameters(), lr=3e-5, eps=1e-08)"]},{"cell_type":"code","execution_count":null,"id":"222fe25f-1bd8-4182-97cb-9c3281b7c347","metadata":{"id":"222fe25f-1bd8-4182-97cb-9c3281b7c347","outputId":"a13ea8c3-15ad-49fd-ef85-401ce7f6e21b"},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/743 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","100%|██████████| 743/743 [1:26:59<00:00,  7.02s/it]\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch:    1] cost = 0.935295391\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 743/743 [1:26:27<00:00,  6.98s/it]\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch:    2] cost = 0.774953568\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 743/743 [1:26:21<00:00,  6.97s/it]\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch:    3] cost = 0.688129196\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 743/743 [1:26:27<00:00,  6.98s/it]\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch:    4] cost = 0.609878846\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 743/743 [1:26:28<00:00,  6.98s/it]\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch:    5] cost = 0.538167688\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 743/743 [1:26:15<00:00,  6.97s/it]\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch:    6] cost = 0.475637002\n"]},{"name":"stderr","output_type":"stream","text":[" 27%|██▋       | 204/743 [22:34<51:33,  5.74s/it]  IOPub message rate exceeded.\n","The Jupyter server will temporarily stop sending output\n","to the client in order to avoid crashing it.\n","To change this limit, set the config variable\n","`--ServerApp.iopub_msg_rate_limit`.\n","\n","Current values:\n","ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n","ServerApp.rate_limit_window=3.0 (secs)\n","\n","100%|██████████| 743/743 [1:26:19<00:00,  6.97s/it]\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch:   10] cost = 0.28240467\n"]}],"source":["for epoch in range(EPOCHS):\n","    model.train()  # 모델을 학습 모드로 설정\n","    epoch_loss = 0\n","\n","    for batch in tqdm.tqdm(data_loader):\n","        # 배치를 직접 장치로 이동\n","        batch = batch.to(device)\n","        labels = batch.clone()\n","\n","        optimizer.zero_grad()\n","\n","        # 모델의 입력과 레이블을 설정\n","        # 이 경우, 입력과 레이블이 같은 텐서를 사용합니다\n","        result = model(input_ids=batch, labels=labels)\n","        loss = result.loss\n","        batch_loss = loss.mean()\n","\n","        batch_loss.backward()\n","        optimizer.step()\n","\n","        epoch_loss += batch_loss.item()\n","\n","    epoch_loss /= len(data_loader)\n","\n","    print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, epoch_loss))\n","\n","    # 얼리 스톱 조건 검사\n","    if epoch_loss < best_loss:\n","        best_loss = epoch_loss\n","        patience_counter = 0  # 성능 개선 시 카운터 초기화\n","    else:\n","        patience_counter += 1  # 성능 개선 없음, 카운터 증가\n","\n","    # Save the model and tokenizer after each epoch\n","    model_save_path = f'model_directory/model_epoch_{epoch + 1}'\n","    model.save_pretrained(model_save_path)\n","    tokenizer.save_pretrained(model_save_path)\n","\n","    if patience_counter >= PATIENCE:\n","        print(\"Early stopping triggered. Training stopped.\")\n","        break"]},{"cell_type":"code","execution_count":null,"id":"e0921240-89f9-4ac6-96b6-7ccb9b2b0721","metadata":{"id":"e0921240-89f9-4ac6-96b6-7ccb9b2b0721","outputId":"8bce52f8-3ea5-47ed-ec0f-713a536956bf"},"outputs":[{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["\n","    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n","Enter your token (input will not be visible): Traceback (most recent call last):\n","  File \"/usr/local/bin/huggingface-cli\", line 8, in <module>\n","    sys.exit(main())\n","  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/commands/huggingface_cli.py\", line 49, in main\n","    service.run()\n","  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/commands/user.py\", line 98, in run\n","    login(token=self.args.token, add_to_git_credential=self.args.add_to_git_credential)\n","  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/_login.py\", line 113, in login\n","    interpreter_login(new_session=new_session, write_permission=write_permission)\n","  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/_login.py\", line 189, in interpreter_login\n","    token = getpass(\"Enter your token (input will not be visible): \")\n","  File \"/usr/lib/python3.10/getpass.py\", line 77, in unix_getpass\n","    passwd = _raw_input(prompt, stream, input=input)\n","  File \"/usr/lib/python3.10/getpass.py\", line 146, in _raw_input\n","    line = input.readline()\n","  File \"/usr/lib/python3.10/codecs.py\", line 319, in decode\n","    def decode(self, input, final=False):\n","KeyboardInterrupt\n"]}],"source":["# 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\n","!huggingface-cli login"]},{"cell_type":"code","execution_count":null,"id":"fbb11430-1aec-4c6f-a9d0-859c76ffd993","metadata":{"id":"fbb11430-1aec-4c6f-a9d0-859c76ffd993","outputId":"e9d70445-3705-42e1-91c1-3b312dc25f96"},"outputs":[{"name":"stdout","output_type":"stream","text":["Enter your Hugging Face token:  ········\n"]},{"name":"stdout","output_type":"stream","text":["env: HF_HOME=/root\n","env: HF_TOKEN=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\n"]},{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["coconut00\n"]}],"source":["import getpass\n","\n","# Hugging Face 토큰을 입력하세요.\n","huggingface_token = getpass.getpass(\"Enter your Hugging Face token: \")\n","\n","# 환경 변수에 토큰 설정\n","%env HF_HOME=/root\n","%env HF_TOKEN=$huggingface_token\n","\n","# 로그인\n","!huggingface-cli whoami\n"]},{"cell_type":"code","execution_count":null,"id":"a626a78f-255a-4081-8a09-19aa8bdaefa5","metadata":{"colab":{"referenced_widgets":["4cd4413949434ffcad8aff407f82af20"]},"id":"a626a78f-255a-4081-8a09-19aa8bdaefa5","outputId":"f757e35e-9e41-4b24-fd23-65cb44ca8576"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4cd4413949434ffcad8aff407f82af20","version_major":2,"version_minor":0},"text/plain":["adapter_model.safetensors:   0%|          | 0.00/40.9M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["CommitInfo(commit_url='https://huggingface.co/coconut00/LDCC_HSTC/commit/e37e60f32fd238c7d842f7c73f743c67ac0bd3d8', commit_message='Upload model', commit_description='', oid='e37e60f32fd238c7d842f7c73f743c67ac0bd3d8', pr_url=None, pr_revision=None, pr_num=None)"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["# 모델 저장 이름!\n","model.push_to_hub(\"LDCC_HSTC\")"]},{"cell_type":"code","execution_count":null,"id":"e7981ad0-5485-4d8a-aa5e-075ea12505fc","metadata":{"colab":{"referenced_widgets":["9383c12582eb48c39d3a0f4fccd2a082"]},"id":"e7981ad0-5485-4d8a-aa5e-075ea12505fc","outputId":"ce409ac5-4842-4358-9b59-5e7aee5f08e2"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9383c12582eb48c39d3a0f4fccd2a082","version_major":2,"version_minor":0},"text/plain":["README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["CommitInfo(commit_url='https://huggingface.co/coconut00/LDCC_HSTC/commit/1b69f122c417e50268cd6650be63de36dbdf2061', commit_message='Upload tokenizer', commit_description='', oid='1b69f122c417e50268cd6650be63de36dbdf2061', pr_url=None, pr_revision=None, pr_num=None)"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["# 토크나이저 저장 이름\n","tokenizer.push_to_hub(\"LDCC_HSTC\")"]},{"cell_type":"code","execution_count":null,"id":"6e85dc08-ff6e-44ad-b6bc-24c8a5ed3891","metadata":{"id":"6e85dc08-ff6e-44ad-b6bc-24c8a5ed3891"},"outputs":[],"source":["def return_answer_by_chatbot(user_text):\n","    sent = '<usr>' + user_text + '<sys>'\n","    input_ids = [tokenizer.bos_token_id] + tokenizer.encode(sent, add_special_tokens=False)\n","    input_ids = torch.tensor([input_ids], dtype=torch.long).to(device)\n","\n","    output = model.generate(input_ids, max_length=150, do_sample=True, top_k=2)\n","    sentence = tokenizer.decode(output[0].tolist())\n","\n","    sys_start = '<sys>'\n","    sys_end = '</s>'\n","\n","    # Find the starting and ending indices of the system response\n","    start_idx = sentence.find(sys_start)\n","    end_idx = sentence.find(sys_end, start_idx + len(sys_start))\n","\n","    # Check if both start and end indices are found\n","    if start_idx != -1 and end_idx != -1:\n","        chatbot_response = sentence[start_idx + len(sys_start):end_idx]\n","        return chatbot_response.strip()\n","    else:\n","        return \"에러: 모델 응답에서 '<sys>'와 '</s>'를 찾을 수 없습니다.\"\n"]},{"cell_type":"code","execution_count":null,"id":"86ebecfd-090d-44fb-a53f-5ba0a5bbcd1f","metadata":{"id":"86ebecfd-090d-44fb-a53f-5ba0a5bbcd1f","outputId":"582d9831-1db6-4405-cfd2-71f22678649a"},"outputs":[{"data":{"text/plain":["'벽지를  붙 일 때는  벽 의 상태 를 잘 점검하여  균 일한 벽면을  만들고 , 필요한  경우 벽지를  부착하기  전에  벽 을 평평하게  만들어 야합니다 . 또한 , 벽지의  패턴을 정확히 맞 추고 공기 방울 이 생기지  않도록  주의하여  벽지를  부착해야  합니다 . 이렇게  하면  벽지가  깔끔하고  아름답게  부착 될 수 있습니다 .'"]},"execution_count":55,"metadata":{},"output_type":"execute_result"}],"source":["return_answer_by_chatbot('도배하는법')"]},{"cell_type":"code","execution_count":null,"id":"d6bb8126-75c6-41d1-a785-d8e303b90654","metadata":{"id":"d6bb8126-75c6-41d1-a785-d8e303b90654","outputId":"1c02ce7c-4aae-45f6-d819-a559113a6aeb"},"outputs":[{"data":{"text/plain":["'누수가  발생했을  때는  빠르게  조치해야  합니다 . 먼 저 물이  누출되는  원인 을 찾아내고 , 그 부분 을 수리하는  작업 을 시작해야  합니다 . 또한 , 물이  누출되는  동안  가구 나 다른  물건을  보호하기  위해  물기가  누출되는  방향 에 대한  방수 조치 를 취해야  합니다 .'"]},"execution_count":57,"metadata":{},"output_type":"execute_result"}],"source":["return_answer_by_chatbot('누수가 생기면 어떻게 해')"]},{"cell_type":"code","execution_count":null,"id":"39b18ce3-6435-42a5-b974-aea438927edd","metadata":{"id":"39b18ce3-6435-42a5-b974-aea438927edd"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":5}
